"""
æ•°æ®è·å–æ¨¡å—
è´Ÿè´£ä»å„ç§æ•°æ®æºè·å–è‚¡ç¥¨æ•°æ®ï¼ŒåŒ…å«ç¼“å­˜é€»è¾‘
"""

import akshare as ak
import pandas as pd
import time
from datetime import datetime, timedelta
from typing import Dict, Tuple, Optional

from utils.logger import get_logger

# å°è¯•å¯¼å…¥é…ç½®ï¼Œå¤±è´¥åˆ™ä½¿ç”¨é»˜è®¤å€¼
try:
    from settings import MEMORY_CACHE_TTL, MAX_CACHE_SIZE, MIN_DATA_DAYS
except ImportError:
    MEMORY_CACHE_TTL = 300
    MAX_CACHE_SIZE = 100
    MIN_DATA_DAYS = 60

logger = get_logger(__name__)

# ==================== å†…å­˜ç¼“å­˜ ====================
_stock_data_cache: Dict[str, Tuple[pd.DataFrame, float]] = {}


def clear_expired_cache():
    """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ¡ç›®"""
    current_time = time.time()
    expired_keys = [
        key for key, (_, cached_time) in _stock_data_cache.items()
        if current_time - cached_time > MEMORY_CACHE_TTL
    ]
    for key in expired_keys:
        del _stock_data_cache[key]
        logger.debug(f"æ¸…ç†è¿‡æœŸç¼“å­˜: {key}")


def is_trading_time() -> bool:
    """åˆ¤æ–­å½“å‰æ˜¯å¦ä¸ºAè‚¡äº¤æ˜“æ—¶æ®µ"""
    now = datetime.now()
    # å‘¨æœ«ä¸äº¤æ˜“
    if now.weekday() >= 5:
        return False
    # äº¤æ˜“æ—¶é—´ï¼š9:30-11:30, 13:00-15:00
    current_time = now.time()
    morning_start = datetime.strptime("09:30", "%H:%M").time()
    morning_end = datetime.strptime("11:30", "%H:%M").time()
    afternoon_start = datetime.strptime("13:00", "%H:%M").time()
    afternoon_end = datetime.strptime("15:00", "%H:%M").time()
    
    return (morning_start <= current_time <= morning_end or 
            afternoon_start <= current_time <= afternoon_end)


def get_realtime_data(stock_code: str) -> Optional[pd.DataFrame]:
    """è·å–ä»Šæ—¥å®æ—¶æ•°æ®ï¼ˆäº¤æ˜“æ—¶æ®µä½¿ç”¨ï¼‰"""
    try:
        df = ak.stock_zh_a_spot_em()
        row = df[df['ä»£ç '] == stock_code]
        if row.empty:
            return None
        
        row = row.iloc[0]
        today = datetime.now().strftime('%Y-%m-%d')
        
        return pd.DataFrame([{
            'date': pd.to_datetime(today),
            'open': float(row['ä»Šå¼€']),
            'high': float(row['æœ€é«˜']),
            'low': float(row['æœ€ä½']),
            'close': float(row['æœ€æ–°ä»·']),
            'volume': float(row['æˆäº¤é‡'])
        }])
    except Exception as e:
        logger.warning(f"è·å–å®æ—¶æ•°æ®å¤±è´¥: {e}")
        return None


def get_stock_data(stock_code: str, days: int = 90, start_date: str = None) -> pd.DataFrame:
    """
    è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå¸¦æœ¬åœ°ç¼“å­˜ + å®æ—¶æ•°æ®ä¼˜åŒ– + è‡ªåŠ¨æ›´æ–°è¿‡æœŸæ•°æ®ï¼‰
    
    ä¼˜å…ˆçº§ï¼š
    1. å†…å­˜ç¼“å­˜ï¼ˆ5åˆ†é’ŸTTLï¼‰
    2. æœ¬åœ°SQLiteæ•°æ®åº“ï¼ˆå†å²æ•°æ®ï¼‰
    3. AkShareç½‘ç»œæ¥å£ï¼ˆå…œåº•ï¼‰
    
    æ–°å¢ï¼šå¦‚æœæœ¬åœ°æ•°æ®è¿‡æœŸï¼ˆè½åäºæœ€è¿‘äº¤æ˜“æ—¥ï¼‰ï¼Œè‡ªåŠ¨ä»ç½‘ç»œè·å–æœ€æ–°æ•°æ®å¹¶åˆå¹¶
    
    Args:
        stock_code: è‚¡ç¥¨ä»£ç 
        days: éœ€è¦çš„å¤©æ•°
        start_date: å¼€å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åŒ…å« date, open, high, low, close, volume åˆ—çš„ DataFrame
    """
    cache_key = f"{stock_code}_{days}_{start_date}"
    current_time = time.time()
    
    # ===== 1. å†…å­˜ç¼“å­˜æ£€æŸ¥ =====
    if cache_key in _stock_data_cache:
        data, cached_time = _stock_data_cache[cache_key]
        if current_time - cached_time < MEMORY_CACHE_TTL:
            logger.info(f"å†…å­˜ç¼“å­˜å‘½ä¸­: {stock_code}")
            return data.copy()
        else:
            logger.debug(f"å†…å­˜ç¼“å­˜è¿‡æœŸ: {stock_code}")
    
    # ===== 2. å°è¯•ä»æœ¬åœ°SQLiteè¯»å– =====
    try:
        from services.local_data_service import get_local_data_service
        local_service = get_local_data_service()
        
        if local_service.has_data(stock_code, min_days=MIN_DATA_DAYS):
            local_data = local_service.get_stock_data(stock_code, days=days)
            
            if local_data is not None and len(local_data) >= MIN_DATA_DAYS:
                logger.info(f"æœ¬åœ°æ•°æ®å‘½ä¸­: {stock_code} ({len(local_data)}å¤©)")
                
                # ğŸ†• æ£€æŸ¥æœ¬åœ°æ•°æ®æ˜¯å¦è¿‡æœŸï¼ˆä¸æœ€è¿‘äº¤æ˜“æ—¥æ¯”è¾ƒï¼‰
                last_date = local_data['date'].max()
                last_trading_date = _get_last_trading_date()
                
                if last_date.date() < last_trading_date:
                    # æœ¬åœ°æ•°æ®è¿‡æœŸï¼Œä»ç½‘ç»œè·å–å¢é‡æ›´æ–°
                    logger.info(f"æœ¬åœ°æ•°æ®è¿‡æœŸ: {stock_code} æœ€æ–°={last_date.date()}, æœ€è¿‘äº¤æ˜“æ—¥={last_trading_date}")
                    try:
                        # ä»æœ¬åœ°æœ€æ–°æ—¥æœŸåä¸€å¤©å¼€å§‹è·å–å¢é‡æ•°æ®
                        incremental_start = (last_date + timedelta(days=1)).strftime('%Y%m%d')
                        incremental_data = _fetch_incremental_data(stock_code, incremental_start)
                        
                        if incremental_data is not None and len(incremental_data) > 0:
                            # åˆå¹¶å¢é‡æ•°æ®
                            local_data = pd.concat([local_data, incremental_data], ignore_index=True)
                            local_data = local_data.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
                            
                            # ä¿å­˜å¢é‡æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“
                            saved_count = local_service.save_stock_data(stock_code, incremental_data)
                            logger.info(f"å¢é‡æ›´æ–°æˆåŠŸ: {stock_code} æ–°å¢ {saved_count} æ¡è®°å½•")
                    except Exception as e:
                        logger.warning(f"å¢é‡æ›´æ–°å¤±è´¥: {stock_code} - {e}")
                
                # äº¤æ˜“æ—¶æ®µï¼šæ‹¼æ¥ä»Šæ—¥å®æ—¶æ•°æ®
                if is_trading_time():
                    today_str = datetime.now().strftime('%Y-%m-%d')
                    last_date_str = local_data['date'].max().strftime('%Y-%m-%d')
                    
                    if last_date_str < today_str:
                        realtime = get_realtime_data(stock_code)
                        if realtime is not None:
                            local_data = pd.concat([local_data, realtime], ignore_index=True)
                            logger.info(f"å·²æ‹¼æ¥å®æ—¶æ•°æ®: {stock_code}")
                
                # é™åˆ¶è¿”å›å¤©æ•°
                if days and len(local_data) > days:
                    local_data = local_data.tail(days).reset_index(drop=True)
                
                _stock_data_cache[cache_key] = (local_data.copy(), current_time)
                return local_data
                
    except Exception as e:
        logger.warning(f"æœ¬åœ°æ•°æ®è¯»å–å¤±è´¥: {e}")
    
    # ===== 3. ä»ç½‘ç»œAPIè·å– =====
    logger.info(f"ä»APIè·å–æ•°æ®: {stock_code}")
    return _fetch_from_network(stock_code, days, start_date, cache_key, current_time)


def _get_last_trading_date() -> datetime.date:
    """è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥æœŸï¼ˆè€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥ï¼‰"""
    now = datetime.now()
    # å¦‚æœæ˜¯å‘¨æœ«ï¼Œå›é€€åˆ°å‘¨äº”
    while now.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
        now -= timedelta(days=1)
    # å¦‚æœå½“å¤©è¿˜æ²¡æ”¶ç›˜ï¼ˆ15:30ä¹‹å‰ï¼‰ï¼Œä½¿ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥
    if now.time() < datetime.strptime("15:30", "%H:%M").time():
        now -= timedelta(days=1)
        while now.weekday() >= 5:
            now -= timedelta(days=1)
    return now.date()


def _fetch_incremental_data(stock_code: str, start_date: str) -> Optional[pd.DataFrame]:
    """è·å–å¢é‡æ•°æ®ï¼ˆä»æŒ‡å®šæ—¥æœŸåˆ°ä»Šå¤©ï¼‰"""
    try:
        end_date = datetime.now().strftime('%Y%m%d')
        data = ak.stock_zh_a_hist(
            symbol=stock_code,
            period="daily",
            start_date=start_date,
            end_date=end_date,
            adjust="qfq"
        )
        
        if data is not None and not data.empty:
            # æ¸…æ´—æ•°æ®
            column_mapping = {
                'æ—¥æœŸ': 'date',
                'å¼€ç›˜': 'open',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æ”¶ç›˜': 'close',
                'æˆäº¤é‡': 'volume'
            }
            for old_col, new_col in column_mapping.items():
                if old_col in data.columns:
                    data = data.rename(columns={old_col: new_col})
            
            required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']
            if all(col in data.columns for col in required_cols):
                data = data[required_cols]
                data['date'] = pd.to_datetime(data['date'])
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    data[col] = pd.to_numeric(data[col], errors='coerce')
                return data.dropna()
        
        return None
    except Exception as e:
        logger.warning(f"è·å–å¢é‡æ•°æ®å¤±è´¥: {stock_code} - {e}")
        return None




def _fetch_from_network(stock_code: str, days: int, start_date: str, 
                        cache_key: str, current_time: float) -> pd.DataFrame:
    """ä»ç½‘ç»œè·å–è‚¡ç¥¨æ•°æ®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    max_retries = 3
    base_retry_delay = 2
    
    for attempt in range(max_retries):
        try:
            if start_date is None:
                start_date = (datetime.now() - timedelta(days=days + 10)).strftime('%Y%m%d')
            
            if attempt > 0:
                delay = base_retry_delay * (2 ** (attempt - 1))
                logger.debug(f"ç­‰å¾… {delay} ç§’åé‡è¯•...")
                time.sleep(delay)
            
            # å°è¯•ä¸œæ–¹è´¢å¯Œ
            data = None
            try:
                if attempt == 0:
                    time.sleep(0.5)
                
                data = ak.stock_zh_a_hist(
                    symbol=stock_code,
                    period="daily",
                    start_date=start_date,
                    adjust="qfq"
                )
            except Exception as em_error:
                logger.warning(f"ä¸œæ–¹è´¢å¯ŒAPIå¤±è´¥ï¼Œå°è¯•è…¾è®¯æ•°æ®æº...")
                try:
                    # è…¾è®¯æ•°æ®æº
                    if stock_code.startswith('6'):
                        tencent_code = f"sh{stock_code}"
                    else:
                        tencent_code = f"sz{stock_code}"
                    
                    end_date = datetime.now().strftime('%Y%m%d')
                    data = ak.stock_zh_a_daily(
                        symbol=tencent_code,
                        start_date=start_date,
                        end_date=end_date,
                        adjust="qfq"
                    )
                except Exception as tx_error:
                    if attempt < max_retries - 1:
                        continue
                    else:
                        raise Exception(
                            f"ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œæ— æ³•è·å–è‚¡ç¥¨ {stock_code} çš„æ•°æ®ã€‚"
                        )
            
            if data is not None and not data.empty:
                data = _clean_data(data, days)
                
                # ===== è‡ªåŠ¨ä¿å­˜åˆ°æœ¬åœ°SQLiteæ•°æ®åº“ =====
                try:
                    from services.local_data_service import get_local_data_service
                    local_service = get_local_data_service()
                    saved_count = local_service.save_stock_data(stock_code, data)
                    if saved_count > 0:
                        logger.info(f"å·²ä¿å­˜åˆ°æœ¬åœ°: {stock_code} ({saved_count}æ¡æ–°è®°å½•)")
                except Exception as save_error:
                    logger.warning(f"ä¿å­˜åˆ°æœ¬åœ°å¤±è´¥: {save_error}")
                
                # æ›´æ–°å†…å­˜ç¼“å­˜
                if len(_stock_data_cache) >= MAX_CACHE_SIZE:
                    clear_expired_cache()
                    if len(_stock_data_cache) >= MAX_CACHE_SIZE:
                        oldest_key = min(_stock_data_cache.keys(), 
                                       key=lambda k: _stock_data_cache[k][1])
                        del _stock_data_cache[oldest_key]
                
                _stock_data_cache[cache_key] = (data.copy(), current_time)
                logger.info(f"å·²ç¼“å­˜: {stock_code} ({len(data)}å¤©)")
                
                return data
            else:
                raise ValueError(f"è·å–çš„æ•°æ®ä¸ºç©º: {stock_code}")

        except Exception as e:
            if attempt < max_retries - 1:
                logger.warning(f"è·å– {stock_code} å¤±è´¥ï¼Œé‡è¯•ä¸­...")
                continue
            else:
                raise
    
    raise Exception(f"è·å–è‚¡ç¥¨ {stock_code} æ•°æ®å¤±è´¥")


def _clean_data(data: pd.DataFrame, days: int) -> pd.DataFrame:
    """æ¸…æ´—æ•°æ®ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    column_mapping = {
        'æ—¥æœŸ': 'date',
        'å¼€ç›˜': 'open',
        'æœ€é«˜': 'high',
        'æœ€ä½': 'low',
        'æ”¶ç›˜': 'close',
        'æˆäº¤é‡': 'volume'
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in data.columns:
            data = data.rename(columns={old_col: new_col})
    
    required_cols = ['date', 'open', 'high', 'low', 'close', 'volume']
    if not all(col in data.columns for col in required_cols):
        raise ValueError("æ•°æ®åˆ—ä¸å®Œæ•´")
    
    data['date'] = pd.to_datetime(data['date'])
    data = data.sort_values('date').reset_index(drop=True)
    
    if days and len(data) > days:
        data = data.tail(days).reset_index(drop=True)
    
    if len(data) < MIN_DATA_DAYS:
        raise ValueError(f"è·å–çš„æ•°æ®ä¸è¶³{MIN_DATA_DAYS}å¤©ï¼Œåªæœ‰{len(data)}å¤©")
    
    numeric_cols = ['open', 'high', 'low', 'close', 'volume']
    for col in numeric_cols:
        data[col] = pd.to_numeric(data[col], errors='coerce')
    
    data = data.dropna(subset=numeric_cols)
    
    if len(data) < MIN_DATA_DAYS:
        raise ValueError(f"æ¸…ç†åæ•°æ®ä¸è¶³{MIN_DATA_DAYS}å¤©")
    
    return data
