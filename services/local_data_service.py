"""
æœ¬åœ°æ•°æ®å­˜å‚¨æœåŠ¡
ä½¿ç”¨ SQLite å­˜å‚¨ Aè‚¡å†å²Kçº¿æ•°æ®ï¼Œæå‡åŠ è½½é€Ÿåº¦
"""

import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, List
import os
import time

# ä½¿ç”¨ç»Ÿä¸€é…ç½®å’Œè£…é¥°å™¨
from services.data_config import (
    REQUEST_TIMEOUT, MAX_RETRIES, RETRY_DELAY, RETRY_BACKOFF,
    DATA_COMPLETENESS_RATIO, API_RATE_LIMIT_DELAY,
    BATCH_SIZE, BATCH_DELAY
)
# ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®æºæ¨¡å—
from services.data_sources import TencentDataSource, AkShareDataSource
from utils.logger import get_logger
from services.data_config import REQUEST_TIMEOUT

logger = get_logger(__name__)


class LocalDataService:
    """æœ¬åœ°æ•°æ®æœåŠ¡ - SQLiteå­˜å‚¨"""
    
    def __init__(self, db_path: str = None):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä¸º data/stock_data.db
        """
        if db_path is None:
            # è·å–é¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent
            db_path = project_root / "data" / "stock_data.db"
        
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®æºï¼ˆä½¿ç”¨ç»Ÿä¸€æ¨¡å—ï¼‰
        self._tencent = TencentDataSource()
        self._akshare = AkShareDataSource()
        
        # åˆå§‹åŒ–æ•°æ®åº“è¡¨
        self._init_db()
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # è‚¡ç¥¨å†å²æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stock_history (
                    code TEXT NOT NULL,
                    date TEXT NOT NULL,
                    open REAL,
                    high REAL,
                    low REAL,
                    close REAL,
                    volume REAL,
                    PRIMARY KEY (code, date)
                )
            ''')
            
            # åŒæ­¥è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS sync_log (
                    code TEXT PRIMARY KEY,
                    last_sync_date TEXT,
                    last_data_date TEXT,
                    record_count INTEGER,
                    updated_at TEXT,
                    full_sync_completed INTEGER DEFAULT 0
                )
            ''')
            
            # å°è¯•æ·»åŠ  full_sync_completed åˆ—ï¼ˆå…¼å®¹æ—§æ•°æ®åº“ï¼‰
            try:
                cursor.execute('ALTER TABLE sync_log ADD COLUMN full_sync_completed INTEGER DEFAULT 0')
            except sqlite3.OperationalError:
                pass  # åˆ—å·²å­˜åœ¨
            
            # åˆ›å»ºç´¢å¼•åŠ é€ŸæŸ¥è¯¢
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_stock_code_date 
                ON stock_history (code, date DESC)
            ''')
            
            conn.commit()
            logger.info(f" æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def save_stock_data(self, code: str, df: pd.DataFrame) -> int:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“ï¼ˆå¢é‡æ›´æ–°ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: åŒ…å« date, open, high, low, close, volume åˆ—çš„ DataFrame
        
        Returns:
            æ–°å¢è®°å½•æ•°
        """
        if df is None or df.empty:
            return 0
        
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ç»Ÿä¸€
        df = df.copy()
        df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
        df['code'] = code
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        columns = ['code', 'date', 'open', 'high', 'low', 'close', 'volume']
        df = df[columns]
        
        # å»é‡ï¼šdataframeå†…éƒ¨å»é‡
        df = df.drop_duplicates(subset=['code', 'date'])
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # 2. è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ•°æ® (ä½†å…è®¸æ›´æ–°å½“å¤©çš„æ•°æ®)
                existing_dates = set()
                cursor.execute("SELECT date FROM stock_history WHERE code = ?", (code,))
                for row in cursor.fetchall():
                     existing_dates.add(row[0])
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»Šå¤©çš„æ•°æ®
                today = datetime.now().strftime('%Y-%m-%d')
                if today in existing_dates and today in df['date'].values:
                    # å¦‚æœåº“é‡Œæœ‰ä»Šå¤©ï¼Œæ–°æ•°æ®ä¹Ÿæœ‰ä»Šå¤©ï¼Œè¯´æ˜éœ€è¦æ›´æ–°å½“å¤©æ•°æ®
                    # å…ˆåˆ é™¤åº“é‡Œçš„ä»Šå¤©æ•°æ®
                    cursor.execute("DELETE FROM stock_history WHERE code = ? AND date = ?", (code, today))
                    existing_dates.remove(today)
                    logger.info(f" {code}: æ›´æ–°å½“æ—¥({today})æ•°æ® (åˆ é™¤æ—§è®°å½•)")
                
                # ~df['date'].isin(existing_dates) è¡¨ç¤ºå–åï¼Œå³ä¸åœ¨ existing_dates ä¸­çš„
                new_data = df[~df['date'].isin(existing_dates)]
                
                if new_data.empty:
                    # logger.info(f" {code}: æ²¡æœ‰æ–°æ•°æ®éœ€è¦ä¿å­˜")
                    return 0
                
                # 3. å†™å…¥æ–°æ•°æ®
                records_before = len(existing_dates)
                new_data.to_sql('stock_history', conn, if_exists='append', index=False,
                          method='multi', chunksize=500)
                
                records_after = records_before + len(new_data)
                
                # 4. æ›´æ–°åŒæ­¥æ—¥å¿—
                last_date = df['date'].max() # ä½¿ç”¨åŸå§‹dfçš„æœ€å¤§æ—¥æœŸï¼Œç¡®ä¿updated_atå‡†ç¡®
                cursor.execute('''
                    INSERT OR REPLACE INTO sync_log 
                    (code, last_sync_date, last_data_date, record_count, updated_at)
                    VALUES (?, ?, ?, ?, ?)
                ''', (code, datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 
                      last_date, records_after, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                
                conn.commit()
                
                logger.info(f" {code}: æ–°å¢ {len(new_data)} æ¡è®°å½• (æ€»è®¡ {records_after} æ¡)")
                return len(new_data)
                
        except Exception as e:
            logger.error(f" ä¿å­˜æ•°æ®å¤±è´¥ {code}: {e}")
            return 0
    
    def get_stock_data(self, code: str, days: int = 90) -> Optional[pd.DataFrame]:
        """
        ä»æœ¬åœ°æ•°æ®åº“è·å–è‚¡ç¥¨å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            days: è·å–æœ€è¿‘Nå¤©çš„æ•°æ®
        
        Returns:
            DataFrame æˆ– Noneï¼ˆæ— æ•°æ®æ—¶ï¼‰
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                query = '''
                    SELECT date, open, high, low, close, volume
                    FROM stock_history
                    WHERE code = ?
                    ORDER BY date DESC
                    LIMIT ?
                '''
                df = pd.read_sql_query(query, conn, params=(code, days))
                
                if df.empty:
                    return None
                
                # è½¬æ¢æ—¥æœŸç±»å‹å¹¶æ’åº
                df['date'] = pd.to_datetime(df['date'])
                df = df.sort_values('date').reset_index(drop=True)
                
                return df
                
        except Exception as e:
            logger.error(f" è¯»å–æœ¬åœ°æ•°æ®å¤±è´¥ {code}: {e}")
            return None
    
    def get_last_data_date(self, code: str) -> Optional[str]:
        """
        è·å–æŸåªè‚¡ç¥¨æœ¬åœ°æ•°æ®çš„æœ€åæ—¥æœŸ
        
        Returns:
            æ—¥æœŸå­—ç¬¦ä¸² 'YYYY-MM-DD' æˆ– None
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                result = conn.execute('''
                    SELECT MAX(date) FROM stock_history WHERE code = ?
                ''', (code,)).fetchone()
                return result[0] if result[0] else None
        except sqlite3.Error as e:
            logger.warning(f"è·å–æœ€åæ•°æ®æ—¥æœŸå¤±è´¥ {code}: {e}")
            return None
    
    def get_first_data_date(self, code: str) -> Optional[str]:
        """
        è·å–æŸåªè‚¡ç¥¨æœ¬åœ°æ•°æ®çš„æœ€æ—©æ—¥æœŸ
        
        Returns:
            æ—¥æœŸå­—ç¬¦ä¸² 'YYYY-MM-DD' æˆ– None
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                result = conn.execute('''
                    SELECT MIN(date) FROM stock_history WHERE code = ?
                ''', (code,)).fetchone()
                return result[0] if result[0] else None
        except sqlite3.Error as e:
            logger.warning(f"è·å–æœ€æ—©æ•°æ®æ—¥æœŸå¤±è´¥ {code}: {e}")
            return None
    
    def get_all_cached_stocks(self) -> List[dict]:
        """
        è·å–æ‰€æœ‰å·²ç¼“å­˜çš„è‚¡ç¥¨åˆ—è¡¨åŠå…¶çŠ¶æ€
        
        Returns:
            [{'code': '600519', 'record_count': 2500, 'last_date': '2024-12-04'}, ...]
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                df = pd.read_sql_query('''
                    SELECT code, record_count, last_data_date as last_date, updated_at
                    FROM sync_log
                    ORDER BY updated_at DESC
                ''', conn)
                return df.to_dict('records')
        except sqlite3.Error as e:
            logger.warning(f"è·å–ç¼“å­˜è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def has_data(self, code: str, min_days: int = 60) -> bool:
        """
        æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            min_days: æœ€å°‘éœ€è¦çš„å¤©æ•°
        
        Returns:
            True/False
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                result = conn.execute('''
                    SELECT COUNT(*) FROM stock_history WHERE code = ?
                ''', (code,)).fetchone()
                return result[0] >= min_days
        except sqlite3.Error as e:
            logger.warning(f"æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨å¤±è´¥ {code}: {e}")
            return False
    
    def get_stats(self) -> dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                total_stocks = conn.execute(
                    "SELECT COUNT(DISTINCT code) FROM stock_history"
                ).fetchone()[0]
                
                total_records = conn.execute(
                    "SELECT COUNT(*) FROM stock_history"
                ).fetchone()[0]
                
                db_size = os.path.getsize(self.db_path) / (1024 * 1024)  # MB
                
                return {
                    "total_stocks": total_stocks,
                    "total_records": total_records,
                    "db_size_mb": round(db_size, 2)
                }
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {"total_stocks": 0, "total_records": 0, "db_size_mb": 0}
    
    # ==================== æ™ºèƒ½æ•°æ®è·å– ====================
    
    def get_last_trading_day(self) -> str:
        """
        è·å–æœ€è¿‘çš„äº¤æ˜“æ—¥
        - å‘¨å…­æ—¥å›é€€åˆ°å‘¨äº”
        - å½“æ—¥15:30å‰è¿”å›æ˜¨å¤©ï¼Œ15:30åè¿”å›ä»Šå¤©
        """
        now = datetime.now()
        today = now.date()
        
        # å¦‚æœå½“å‰æ˜¯15:30å‰ï¼Œæœ€åäº¤æ˜“æ—¥æ˜¯æ˜¨å¤©
        if now.hour < 15 or (now.hour == 15 and now.minute < 30):
            target = today - timedelta(days=1)
        else:
            target = today
        
        # è·³è¿‡å‘¨æœ«
        while target.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
            target -= timedelta(days=1)
        
        return target.strftime('%Y-%m-%d')
    
    def needs_update(self, last_date: Optional[str]) -> bool:
        """
        åˆ¤æ–­æ•°æ®æ˜¯å¦éœ€è¦æ›´æ–°
        
        è§„åˆ™ï¼š
        - å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œéœ€è¦æ›´æ–°
        - å¦‚æœæœ€åæ•°æ®æ—¥æœŸ < æœ€è¿‘äº¤æ˜“æ—¥ï¼Œéœ€è¦æ›´æ–°
        """
        if last_date is None:
            return True
        
        last_trading_day = self.get_last_trading_day()
        
        # å¦‚æœæ•°æ®æ—¥æœŸ < æœ€è¿‘äº¤æ˜“æ—¥ï¼Œè‚¯å®šè¦æ›´æ–°
        if last_date < last_trading_day:
            return True
        
        # å¦‚æœæ•°æ®æ—¥æœŸ == æœ€è¿‘äº¤æ˜“æ—¥ï¼ˆå³ä»Šå¤©ï¼‰ï¼Œæš‚ä¸éœ€è¦æ›´æ–°
        # é™ˆæ—§æ•°æ®æ£€æŸ¥ç”± _is_data_stale æ–¹æ³•å¤„ç†
                
        return False
    
    def is_full_sync_completed(self, code: str) -> bool:
        """æ£€æŸ¥è¯¥è‚¡ç¥¨æ˜¯å¦å·²å®Œæˆå…¨é‡åŒæ­¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                result = conn.execute(
                    "SELECT full_sync_completed FROM sync_log WHERE code = ?", (code,)
                ).fetchone()
                return result is not None and result[0] == 1
        except sqlite3.Error as e:
            logger.warning(f"æ£€æŸ¥å…¨é‡åŒæ­¥çŠ¶æ€å¤±è´¥ {code}: {e}")
            return False
    
    def mark_full_sync_completed(self, code: str):
        """æ ‡è®°è¯¥è‚¡ç¥¨å·²å®Œæˆå…¨é‡åŒæ­¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute(
                    "UPDATE sync_log SET full_sync_completed = 1 WHERE code = ?", (code,)
                )
                conn.commit()
        except Exception as e:
            logger.warning(f"æ ‡è®°å…¨é‡åŒæ­¥å¤±è´¥ {code}: {e}")

    def _is_data_stale(self, code: str) -> bool:
        """
        æ£€æŸ¥æ•°æ®æ˜¯å¦é™ˆæ—§ï¼ˆå³ï¼šä»Šå¤©æ˜¯äº¤æ˜“æ—¥ï¼Œä¸”åº“é‡Œæœ‰ä»Šå¤©æ•°æ®ï¼Œä½†æ˜¯æ˜¯åœ¨æ”¶ç›˜å‰æ›´æ–°çš„ï¼‰
        """
        try:
            # 1. å¦‚æœç°åœ¨è¿˜åœ¨äº¤æ˜“ç›˜ä¸­ï¼Œä¸éœ€è¦è®¤ä¸ºé™ˆæ—§ï¼ˆå› ä¸ºæœ¬æ¥å°±æ˜¯å˜åŠ¨çš„ï¼‰
            from utils.date_utils import is_trading_time
            if is_trading_time():
                return False
                
            # 2. å¦‚æœå·²ç»æ”¶ç›˜
            # æ£€æŸ¥åŒæ­¥æ—¥å¿—
            with sqlite3.connect(self.db_path) as conn:
                row = conn.execute(
                    "SELECT updated_at, last_data_date FROM sync_log WHERE code = ?", (code,)
                ).fetchone()
                
                if not row:
                    return False
                    
                updated_at_str, last_date_str = row
                
                # å¦‚æœæœ€åæ•°æ®æ—¥æœŸä¸æ˜¯ä»Šå¤©ï¼Œé‚£ç”± needs_update å¤„ç†ï¼Œè¿™é‡Œä¸ç®¡
                today_str = datetime.now().strftime('%Y-%m-%d')
                if last_date_str != today_str:
                    return False
                    
                # å¦‚æœæœ€åæ•°æ®æ˜¯ä»Šå¤©ï¼Œä¸” updated_at æ—©äºä»Šå¤©çš„ 15:00
                updated_at = datetime.strptime(updated_at_str, '%Y-%m-%d %H:%M:%S')
                market_close_time = datetime.now().replace(hour=15, minute=0, second=0, microsecond=0)
                
                # å¦‚æœæ›´æ–°æ—¶é—´ < ä»Šå¤©15:00ï¼Œä¸” ç°åœ¨æ—¶é—´ > 15:00
                if updated_at < market_close_time and datetime.now() > market_close_time:
                    logger.info(f" {code}: æ•°æ®é™ˆæ—§ (æ›´æ–°äº {updated_at_str}, æ”¶ç›˜å‰)ï¼Œå¼ºåˆ¶æ›´æ–°")
                    return True
                    
        except Exception as e:
            logger.warning(f"æ£€æŸ¥æ•°æ®é™ˆæ—§å¤±è´¥ {code}: {e}")
            
        return False

    def get_stock_data_smart(self, code: str, days: int = 90, include_realtime: bool = True) -> Optional[pd.DataFrame]:
        """
        æ™ºèƒ½è·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆç»Ÿä¸€å…¥å£ï¼Œå¸¦å®æ—¶èåˆï¼‰
        
        æµç¨‹ï¼š
        1. æ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰è¶³å¤Ÿæ•°æ®
        2. å¦‚æœæœ‰ï¼šç«‹å³è¿”å› + åå°å¼‚æ­¥æ›´æ–°/è¡¥å…¨
        3. å¦‚æœæ²¡æœ‰ï¼šåŒæ­¥è·å–åˆå§‹æ•°æ® + åå°å¼‚æ­¥è¡¥å…¨å†å²
        4. äº¤æ˜“æ—¶æ®µè‡ªåŠ¨èåˆå®æ—¶æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            days: éœ€è¦çš„å¤©æ•°
            include_realtime: æ˜¯å¦èåˆå®æ—¶æ•°æ®ï¼ˆé»˜è®¤Trueï¼‰
        """
        from services.background_tasks import submit_background_task, TaskPriority
        from services.data_config import DATA_COMPLETENESS_RATIO, MAX_RETRIES
        from utils.date_utils import is_trading_time
        
        # 1. å°è¯•ä»æœ¬åœ°è·å–
        local_data = self.get_stock_data(code, days=days)
        
        if local_data is not None and len(local_data) >= int(days * DATA_COMPLETENESS_RATIO):
            # æœ¬åœ°æ•°æ®å……è¶³ï¼Œç«‹å³è¿”å›
            logger.info(f"âœ… {code}: æœ¬åœ°æ•°æ®å……è¶³ ({len(local_data)} æ¡)")
            
            # åå°å¢é‡æ›´æ–°ï¼ˆé«˜ä¼˜å…ˆçº§ - ç”¨æˆ·éœ€è¦æœ€æ–°æ•°æ®ï¼‰
            last_date = self.get_last_data_date(code)
            if last_date and (self.needs_update(last_date) or self._is_data_stale(code)):
                submit_background_task(
                    self._background_incremental_update,
                    code,
                    task_name=f"å¢é‡æ›´æ–°-{code}",
                    priority=TaskPriority.HIGH  # é«˜ä¼˜å…ˆçº§
                )
            
            # åå°è¡¥å…¨å†å²æ•°æ®ï¼ˆä½ä¼˜å…ˆçº§ - ä¸ç€æ€¥ï¼‰
            if not self.is_full_sync_completed(code):
                submit_background_task(
                    self._background_backfill,
                    code,
                    task_name=f"å†å²è¡¥å…¨-{code}",
                    priority=TaskPriority.LOW  # ä½ä¼˜å…ˆçº§
                )
            
            # èåˆå®æ—¶æ•°æ®ï¼ˆäº¤æ˜“æ—¶æ®µï¼‰
            if include_realtime:
                local_data = self._merge_realtime_data(code, local_data)
            
            return local_data
        
        # 2. æœ¬åœ°æ•°æ®ä¸è¶³ï¼ŒåŒæ­¥è·å–åˆå§‹æ•°æ®ï¼ˆå¸¦å¤šæºFallbackï¼‰
        logger.info(f"âš ï¸ {code}: æœ¬åœ°æ•°æ®ä¸è¶³ï¼Œè·å–åˆå§‹æ•°æ®...")
        
        initial_data = None
        
        # ä¼˜å…ˆä½¿ç”¨è…¾è®¯
        for attempt in range(MAX_RETRIES):
            initial_data = self._tencent.fetch_kline(code, days=days)
            if initial_data is not None and not initial_data.empty:
                logger.info(f"âœ… {code}: è…¾è®¯è·å–æˆåŠŸ ({len(initial_data)} æ¡)")
                break
            logger.warning(f"âŒ {code}: è…¾è®¯è·å–å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{MAX_RETRIES}")
            time.sleep(1)
        
        # è…¾è®¯å¤±è´¥ï¼Œå°è¯•ä¸œè´¢
        if initial_data is None or initial_data.empty:
            logger.info(f"ğŸ”„ {code}: è…¾è®¯å¤±è´¥ï¼Œå°è¯•ä¸œè´¢...")
            from services.data_sources import EastmoneyDataSource
            eastmoney = EastmoneyDataSource()
            initial_data = eastmoney.fetch_kline(code, days=min(days, 3000))
            if initial_data is not None and not initial_data.empty:
                logger.info(f"âœ… {code}: ä¸œè´¢è·å–æˆåŠŸ ({len(initial_data)} æ¡)")
        
        # ä¸œè´¢ä¹Ÿå¤±è´¥ï¼Œå°è¯•AkShare
        if initial_data is None or initial_data.empty:
            logger.info(f"ğŸ”„ {code}: ä¸œè´¢å¤±è´¥ï¼Œå°è¯•AkShare...")
            initial_data = self._akshare.fetch_kline(code, days=days)
            if initial_data is not None and not initial_data.empty:
                logger.info(f"âœ… {code}: AkShareè·å–æˆåŠŸ ({len(initial_data)} æ¡)")
        
        if initial_data is not None and not initial_data.empty:
            self.save_stock_data(code, initial_data)
            logger.info(f"âœ… {code}: åˆå§‹æ•°æ®ä¿å­˜å®Œæˆ ({len(initial_data)} æ¡)")
            
            # åå°è¡¥å…¨å†å²ï¼ˆä½ä¼˜å…ˆçº§ï¼‰
            submit_background_task(
                self._background_backfill,
                code,
                task_name=f"å†å²è¡¥å…¨-{code}",
                priority=TaskPriority.LOW
            )
            
            # èåˆå®æ—¶æ•°æ®
            if include_realtime:
                initial_data = self._merge_realtime_data(code, initial_data)
            
            return initial_data
        
        logger.warning(f"âŒ {code}: æ— æ³•è·å–æ•°æ®ï¼ˆé‡è¯• {MAX_RETRIES} æ¬¡åå¤±è´¥ï¼‰")
        return None
    
    def _background_incremental_update(self, code: str):
        """åå°å¢é‡æ›´æ–°ä»»åŠ¡"""
        logger.info(f"[åå°ä»»åŠ¡] {code}: å¼€å§‹å¢é‡æ›´æ–°")
        try:
            last_date = self.get_last_data_date(code)
            if not last_date:
                return
            
            # å¦‚æœæ˜¯é™ˆæ—§æ•°æ®ï¼Œä»å‰ä¸€å¤©å¼€å§‹è·å–
            if self._is_data_stale(code):
                fetch_start = (datetime.strptime(last_date, '%Y-%m-%d') - timedelta(days=1)).strftime('%Y-%m-%d')
            else:
                fetch_start = last_date
            
            new_data = self._fetch_incremental(code, fetch_start)
            if new_data is not None and not new_data.empty:
                self.save_stock_data(code, new_data)
                logger.info(f"[åå°ä»»åŠ¡] {code}: å¢é‡æ›´æ–°å®Œæˆ ({len(new_data)} æ¡)")
        except Exception as e:
            logger.error(f"[åå°ä»»åŠ¡] {code}: å¢é‡æ›´æ–°å¤±è´¥ - {e}")
    
    def _background_backfill(self, code: str):
        """åå°è¡¥å…¨å†å²æ•°æ®ä»»åŠ¡"""
        logger.info(f"[åå°ä»»åŠ¡] {code}: å¼€å§‹è¡¥å…¨å†å²æ•°æ®")
        try:
            from services.data_config import BACKFILL_MAX_ITERATIONS, API_RATE_LIMIT_DELAY
            
            first_date = self.get_first_data_date(code)
            if not first_date:
                logger.warning(f"[åå°ä»»åŠ¡] {code}: æ— æ³•è·å–æœ€æ—©æ—¥æœŸ")
                return
            
            # æ¸è¿›å¼è¡¥å…¨
            for i in range(BACKFILL_MAX_ITERATIONS):
                backward_data = self._fetch_backward_from_tencent(code, first_date)
                
                if backward_data is None or backward_data.empty:
                    logger.info(f"[åå°ä»»åŠ¡] {code}: å·²åˆ°è¾¾æœ€æ—©å¯ç”¨æ•°æ®")
                    break
                
                self.save_stock_data(code, backward_data)
                first_date = self.get_first_data_date(code)
                logger.info(f"[åå°ä»»åŠ¡] {code}: è¡¥å…¨ç¬¬ {i+1} æ‰¹ ({len(backward_data)} æ¡)")
                
                time.sleep(API_RATE_LIMIT_DELAY * 2)  # é¿å…è¯·æ±‚è¿‡å¿«
            
            # æ ‡è®°å®Œæˆ
            self.mark_full_sync_completed(code)
            logger.info(f"[åå°ä»»åŠ¡] {code}: å†å²æ•°æ®è¡¥å…¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"[åå°ä»»åŠ¡] {code}: è¡¥å…¨å¤±è´¥ - {e}")
    
    def _merge_realtime_data(self, code: str, data: pd.DataFrame) -> pd.DataFrame:
        """
        èåˆå®æ—¶æ•°æ®ï¼ˆäº¤æ˜“æ—¶æ®µä½¿ç”¨ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            data: å†å²æ•°æ® DataFrame
        
        Returns:
            èåˆåçš„ DataFrame
        """
        from utils.date_utils import is_trading_time
        from datetime import datetime
        
        if not is_trading_time():
            return data
        
        try:
            from services.realtime_quotation_service import get_realtime_service
            
            today_str = datetime.now().strftime('%Y-%m-%d')
            last_date_str = data['date'].max().strftime('%Y-%m-%d')
            
            if last_date_str >= today_str:
                return data  # å·²æœ‰ä»Šæ—¥æ•°æ®
            
            # è·å–å®æ—¶è¡Œæƒ…
            service = get_realtime_service()
            quote = service.get_realtime_with_fallback(code)
            
            if not quote or code not in quote:
                return data
            
            realtime = quote[code]
            realtime_row = pd.DataFrame([{
                'date': pd.to_datetime(today_str),
                'open': float(realtime.get('open', 0)),
                'high': float(realtime.get('high', 0)),
                'low': float(realtime.get('low', 0)),
                'close': float(realtime.get('now', 0)),
                'volume': float(realtime.get('volume', 0) or realtime.get('turnover', 0))
            }])
            
            data = pd.concat([data, realtime_row], ignore_index=True)
            logger.debug(f"å·²èåˆå®æ—¶æ•°æ®: {code}")
            
        except Exception as e:
            logger.warning(f"èåˆå®æ—¶æ•°æ®å¤±è´¥ {code}: {e}")
        
        return data

    
    def _fetch_full_history(self, code: str, days: int = 365) -> Optional[pd.DataFrame]:
        """
        ä»ç½‘ç»œè·å–å®Œæ•´å†å²æ•°æ®ï¼ˆä½¿ç”¨ç»Ÿä¸€æ•°æ®æºæ¨¡å—ï¼‰
        
        æ•°æ®æºä¼˜å…ˆçº§:
        - å¦‚æœ days <= 640: Tencent (å¿«ä¸”ç¨³) â†’ AkShare
        - å¦‚æœ days > 640: AkShare (å…¨é‡) â†’ Tencent (å…œåº•)
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            days: éœ€è¦çš„å¤©æ•° (9999 è¡¨ç¤ºå…¨é‡)
        
        Returns:
            DataFrame æˆ– None
        """
        # å…¨é‡åŒæ­¥åœºæ™¯ (days > 640): è…¾è®¯æ¥å£æœ€å¤š640å¤©ï¼Œç”¨ AkShare
        if days > 640:
            logger.info(f" [å…¨é‡] {code}: éœ€è¦ {days} å¤©ï¼Œä½¿ç”¨ AkShare è·å–å†å²...")
            
            # 1. AkShareï¼ˆæ”¯æŒè¾ƒé•¿å†å²ï¼‰
            df = self._akshare.fetch_kline(code, days)
            if df is not None:
                return df
            
            # 2. è…¾è®¯å…œåº•ï¼ˆè™½ç„¶åªæœ‰640å¤©ï¼‰
            df = self._tencent.fetch_kline(code, 640)
            if df is not None:
                logger.warning(f" [å…¨é‡] {code}: ä»…è·å–åˆ°è…¾è®¯640å¤©æ•°æ®")
                return df
        else:
            # å¸¸è§„åœºæ™¯: ä¼˜å…ˆè…¾è®¯ï¼ˆå¿«ï¼‰
            df = self._tencent.fetch_kline(code, days)
            if df is not None:
                return df
            
            # AkShare å¤‡ç”¨
            df = self._akshare.fetch_kline(code, days)
            if df is not None:
                return df
        
        logger.error(f" [full_history] {code} æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥")
        return None
    
    # ==================== ä»¥ä¸‹æ–¹æ³•å·²è¿ç§»è‡³ data_sources æ¨¡å— ====================
    # _fetch_from_akshare â†’ AkShareDataSource.fetch_kline()
    # _fetch_from_tencent_qfq â†’ TencentDataSource.fetch_kline()
    # _fetch_from_tushare â†’ å·²ç§»é™¤ï¼ˆä¸å†ä½¿ç”¨ï¼‰

    def _fetch_backward_from_tencent(self, code: str, first_date: str) -> Optional[pd.DataFrame]:
        """
        ä»è…¾è®¯è·å–æŒ‡å®šæ—¥æœŸä¹‹å‰çš„å†å²æ•°æ®ï¼ˆå‘å‰è¡¥å…¨ï¼‰
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            first_date: æœ¬åœ°æœ€æ—©æ—¥æœŸ (YYYY-MM-DD)ï¼Œè·å–è¯¥æ—¥æœŸä¹‹å‰çš„æ•°æ®
        
        Returns:
            DataFrame æˆ– None
        """
        try:
            import requests
            from datetime import datetime, timedelta
            
            # è®¡ç®—ç»“æŸæ—¥æœŸï¼ˆfirst_date çš„å‰ä¸€å¤©ï¼‰
            end_dt = datetime.strptime(first_date, '%Y-%m-%d') - timedelta(days=1)
            end_date = end_dt.strftime('%Y-%m-%d')
            
            # è½¬æ¢ä»£ç å‰ç¼€
            if code.startswith(("5", "6", "9")):
                symbol = "sh" + code
            else:
                symbol = "sz" + code
            
            # è…¾è®¯æ¥å£æ”¯æŒæŒ‡å®šæ—¥æœŸåŒºé—´ï¼šparam=code,day,start,end,count,qfq
            # è¿™é‡Œæˆ‘ä»¬åªæŒ‡å®š end_dateï¼Œè·å– end_date ä¹‹å‰çš„ 640 å¤©
            url = f"http://web.ifzq.gtimg.cn/appstock/app/fqkline/get?param={symbol},day,,{end_date},640,qfq"
            
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Referer": "http://gu.qq.com/"
            }
            
            resp = requests.get(url, headers=headers, timeout=REQUEST_TIMEOUT)
            data = resp.json()
            
            if not data.get('data') or symbol not in data['data']:
                return None
            
            stock_data = data['data'][symbol]
            klines = stock_data.get('qfqday', stock_data.get('day'))
            
            if not klines or len(klines) == 0:
                return None
            
            # è§£ææ•°æ®
            records = []
            for row in klines:
                if len(row) < 6:
                    continue
                record_date = row[0]
                # è¿‡æ»¤æ‰ >= first_date çš„æ•°æ®ï¼ˆé˜²æ­¢é‡å¤ï¼‰
                if record_date >= first_date:
                    continue
                records.append({
                    "date": record_date,
                    "open": float(row[1]),
                    "close": float(row[2]),
                    "high": float(row[3]),
                    "low": float(row[4]),
                    "volume": float(row[5])
                })
            
            if not records:
                return None
            
            df = pd.DataFrame(records)
            logger.info(f" [å‘å‰] {code} è·å– {len(df)} æ¡æ›´æ—©è®°å½• (æˆªè‡³ {end_date})")
            return df
            
        except Exception as e:
            # å‘å‰è¡¥å…¨å¤±è´¥ä¸æ˜¯ä¸¥é‡é”™è¯¯ï¼Œé™é»˜å¤„ç†
            return None

    def _fetch_incremental(self, code: str, last_date: str) -> Optional[pd.DataFrame]:
        """
        è·å–å¢é‡æ•°æ®ï¼ˆä»æŒ‡å®šæ—¥æœŸåˆ°ä»Šå¤©ï¼‰
        ä¼˜å…ˆçº§: Tencent > AkShare
        """
        import akshare as ak
        
        # 0. å…ˆå°è¯•è…¾è®¯ (å¿«ä¸”ç¨³)
        # è®¡ç®—å¤§è‡´éœ€è¦çš„å¤©æ•°
        try:
            last = datetime.strptime(last_date, '%Y-%m-%d')
            delta = (datetime.now() - last).days
            if delta > 0:
                # å¤šè¯·æ±‚ä¸€ç‚¹ä»¥é˜²ä¸‡ä¸€
                df = self._tencent.fetch_kline(code, days=delta + 10)
                if df is not None and not df.empty:
                    # è¿‡æ»¤å‡º last_date ä¹‹åçš„æ•°æ®
                    df = df[df['date'] > last_date]
                    if not df.empty:
                        logger.info(f" [å¢é‡] Tencent {code} æˆåŠŸè·å– {len(df)} æ¡æ–°è®°å½•")
                        return df
        except Exception as e:
            logger.warning(f" [å¢é‡] Tencent å°è¯•å¤±è´¥: {e}")

        # ... å¦‚æœè…¾è®¯å¤±è´¥ï¼Œå›é€€åˆ° AkShare ...
        
        # ä»last_dateåä¸€å¤©å¼€å§‹è·å–
        start = datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)
        end = datetime.now()
        
        # å¦‚æœèµ·å§‹æ—¥æœŸå·²ç»è¶…è¿‡ä»Šå¤©ï¼Œæ— éœ€æ›´æ–°
        if start.date() > end.date():
            return None
        
        max_retries = 3
        delay = 2.0
        
        for attempt in range(max_retries + 1):
            try:
                df = ak.stock_zh_a_hist(
                    symbol=code,
                    period='daily',
                    start_date=start.strftime('%Y%m%d'),
                    end_date=end.strftime('%Y%m%d'),
                    adjust='qfq'
                )
                
                if df is None or df.empty:
                    return None
                
                # é‡å‘½ååˆ—
                df = df.rename(columns={
                    'æ—¥æœŸ': 'date',
                    'å¼€ç›˜': 'open',
                    'æœ€é«˜': 'high',
                    'æœ€ä½': 'low',
                    'æ”¶ç›˜': 'close',
                    'æˆäº¤é‡': 'volume'
                })
                
                df = df[['date', 'open', 'high', 'low', 'close', 'volume']]
                
                logger.info(f" [å¢é‡] AkShare {code} è·å– {len(df)} æ¡æ–°è®°å½•")
                return df
                
            except Exception as e:
                if attempt < max_retries:
                    logger.warning(f" [å¢é‡] AkShare {code} è·å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    # print(f"   ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    delay *= 2  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f" [å¢é‡] AkShare {code} æœ€ç»ˆå¤±è´¥: {e}")
                    return None
        
        return None
    
    def update_all_cached_stocks(self, batch_size: int = 50, delay: float = 2.0):
        """
        æ›´æ–°æ‰€æœ‰å·²ç¼“å­˜è‚¡ç¥¨çš„æ•°æ®ï¼ˆæ”¶ç›˜åæ‰¹é‡æ›´æ–°ï¼‰
        
        Args:
            batch_size: æ¯æ‰¹æ›´æ–°çš„è‚¡ç¥¨æ•°é‡
            delay: æ¯æ‰¹ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼ˆä¿æŠ¤IPï¼‰
        """
        import time
        
        cached_stocks = self.get_all_cached_stocks()
        if not cached_stocks:
            logger.info("æ²¡æœ‰éœ€è¦æ›´æ–°çš„è‚¡ç¥¨")
            return
        
        logger.info(f" å¼€å§‹æ›´æ–° {len(cached_stocks)} åªè‚¡ç¥¨...")
        
        updated_count = 0
        for i, stock in enumerate(cached_stocks):
            code = stock['code']
            last_date = stock.get('last_date')
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if not self.needs_update(last_date):
                continue
            
            try:
                new_data = self._fetch_incremental(code, last_date)
                if new_data is not None and not new_data.empty:
                    self.save_stock_data(code, new_data)
                    updated_count += 1
            except Exception as e:
                logger.error(f" æ›´æ–° {code} å¤±è´¥: {e}")
            
            # æ¯æ‰¹æ¬¡åå»¶è¿Ÿï¼Œä¿æŠ¤IP
            if (i + 1) % batch_size == 0:
                logger.info(f" å·²å¤„ç† {i + 1}/{len(cached_stocks)}ï¼Œä¼‘æ¯ {delay} ç§’...")
                time.sleep(delay)
        
        logger.info(f" æ›´æ–°å®Œæˆï¼Œå…±æ›´æ–° {updated_count} åªè‚¡ç¥¨")


# å…¨å±€å•ä¾‹
_local_data_service = None

def get_local_data_service() -> LocalDataService:
    """è·å–æœ¬åœ°æ•°æ®æœåŠ¡å•ä¾‹"""
    global _local_data_service
    if _local_data_service is None:
        _local_data_service = LocalDataService()
    return _local_data_service
